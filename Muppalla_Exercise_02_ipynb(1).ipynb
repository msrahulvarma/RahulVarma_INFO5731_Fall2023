{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msrahulvarma/RahulVarma_INFO5731_Fall2023/blob/main/Muppalla_Exercise_02_ipynb(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW5_oFVd9-pY"
      },
      "source": [
        "## The second In-class-exercise (09/13/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kindly use the provided .ipynb document to write your code or respond to the questions. Avoid generating a new file.\n",
        "Execute all the cells before your final submission."
      ],
      "metadata": {
        "id": "mAzh1U0sE5I5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This in-class exercise is due tomorrow September 14, 2023 at 11:59 PM. No late submissions will be considered."
      ],
      "metadata": {
        "id": "PpgvZQdRE-HV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QBZI-je9-pZ"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWoKpYQT9-pa"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-LmNR3kw9-pa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "0b39d74b-48f7-4d18-82c4-122759023986"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nResearch Question:\\nWhat is the range of warranty durations offered for the smartphones listed on the website?\\nData Required to Answer the Questions:\\nProduct Name: To identify each product uniquely.\\nType/Specification: To categorize the products based on their features or specifications.\\nWarranty Info: To analyze the warranty duration offered for each product.\\nDiscount Price: To understand the price after the discount.\\nOriginal Price: To compare with the discounted price to calculate the discount amount or percentage.\\nHow Many Data Needed for the Analysis:\\nA sample of at least 30-50 data points (products) would be ideal to conduct a preliminary analysis.\\nFor a more comprehensive analysis, collecting data from multiple pages (if available) to gather information on several hundred products would be beneficial.\\nDetailed Steps for Collecting and Saving the Data:\\nIdentify the Source: Identify the website or webpage (in this case, www.backmarket.com) from which the data needs to be scraped.\\nSend HTTP Request: Use the requests library in Python to send an HTTP GET request to the webpage.\\nParse HTML Content: Utilize the BeautifulSoup library to parse the HTML content received from the HTTP request.\\nIdentify HTML Tags & Classes: Inspect the HTML structure of the webpage and identify the tags and classes where the required data is located.\\nExtract Data: Iterate over the identified tags and classes and extract the required data points, such as product name, type/specification, warranty info, discount price, and original price.\\nHandle Exceptions: Use try-except blocks to handle any exceptions or errors that may occur during data extraction, and assign a default value (e.g., 'N/A') if the data is not available.\\nStore Data in DataFrame: Create a pandas DataFrame and store the extracted data in respective columns.\\nSave DataFrame to Excel: Utilize the to_excel method of the pandas DataFrame to save the collected data to an Excel file.\\nRequirements:\\nPython: The programming language used to write the script.\\nPython Libraries: requests for sending HTTP requests, BeautifulSoup from bs4 for parsing HTML, and pandas for creating DataFrames and saving them to Excel.\\nWeb Page Structure Knowledge: Basic understanding of HTML tags and classes to identify where the required data is located on the webpage.\\nInternet Connection: To send HTTP requests to the website and receive responses.\\nExcel or Compatible Software: To view the saved Excel file containing the collected data.\\nEthical Consideration: Ensure compliance with the website’s robots.txt file or Terms of Service regarding web scraping.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "\n",
        "Research Question:\n",
        "What is the range of warranty durations offered for the smartphones listed on the website?\n",
        "Data Required to Answer the Questions:\n",
        "Product Name: To identify each product uniquely.\n",
        "Type/Specification: To categorize the products based on their features or specifications.\n",
        "Warranty Info: To analyze the warranty duration offered for each product.\n",
        "Discount Price: To understand the price after the discount.\n",
        "Original Price: To compare with the discounted price to calculate the discount amount or percentage.\n",
        "How Many Data Needed for the Analysis:\n",
        "A sample of at least 30-50 data points (products) would be ideal to conduct a preliminary analysis.\n",
        "For a more comprehensive analysis, collecting data from multiple pages (if available) to gather information on several hundred products would be beneficial.\n",
        "Detailed Steps for Collecting and Saving the Data:\n",
        "Identify the Source: Identify the website or webpage (in this case, www.backmarket.com) from which the data needs to be scraped.\n",
        "Send HTTP Request: Use the requests library in Python to send an HTTP GET request to the webpage.\n",
        "Parse HTML Content: Utilize the BeautifulSoup library to parse the HTML content received from the HTTP request.\n",
        "Identify HTML Tags & Classes: Inspect the HTML structure of the webpage and identify the tags and classes where the required data is located.\n",
        "Extract Data: Iterate over the identified tags and classes and extract the required data points, such as product name, type/specification, warranty info, discount price, and original price.\n",
        "Handle Exceptions: Use try-except blocks to handle any exceptions or errors that may occur during data extraction, and assign a default value (e.g., 'N/A') if the data is not available.\n",
        "Store Data in DataFrame: Create a pandas DataFrame and store the extracted data in respective columns.\n",
        "Save DataFrame to Excel: Utilize the to_excel method of the pandas DataFrame to save the collected data to an Excel file.\n",
        "Requirements:\n",
        "Python: The programming language used to write the script.\n",
        "Python Libraries: requests for sending HTTP requests, BeautifulSoup from bs4 for parsing HTML, and pandas for creating DataFrames and saving them to Excel.\n",
        "Web Page Structure Knowledge: Basic understanding of HTML tags and classes to identify where the required data is located on the webpage.\n",
        "Internet Connection: To send HTTP requests to the website and receive responses.\n",
        "Excel or Compatible Software: To view the saved Excel file containing the collected data.\n",
        "Ethical Consideration: Ensure compliance with the website’s robots.txt file or Terms of Service regarding web scraping.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlxTLRNm9-pa"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QpWOgjHi9-pa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd55c9e7-8764-496d-8d92-fad59bd0ab7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been saved to extracted_data.xlsx\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "# Initialize empty lists to store the data\n",
        "names = []\n",
        "types = []\n",
        "warranties = []\n",
        "discount_prices = []\n",
        "original_prices = []\n",
        "\n",
        "# Loop through a range of pages (here, only one page for demonstration)\n",
        "for num in range(1, 2):\n",
        "    # Construct the URL for the page\n",
        "    page_url = f'https://www.backmarket.com/en-us/l/smartphones/0744fd27-8605-465d-8691-3b6dffda5969?page={num}#'\n",
        "\n",
        "    # Send a GET request to the website\n",
        "    res = requests.get(page_url)\n",
        "\n",
        "    # Parse the HTML content of the page\n",
        "    page_soup = bs(res.content, 'html.parser')\n",
        "\n",
        "    # Find all product data elements on the page\n",
        "    products = page_soup.find_all('div', {'class': 'flex flex-col md:flex-1 md:justify-end'})\n",
        "\n",
        "    # Iterate through the product elements to extract information\n",
        "    for item in products:\n",
        "        try:\n",
        "            # Get the product name\n",
        "            name = item.find('h2').get_text()\n",
        "        except:\n",
        "            name = 'N/A'\n",
        "\n",
        "        try:\n",
        "            # Get the product type/specification\n",
        "            type_spec = item.find('span', {'class': 'body-2-light duration-200 line-clamp-1 normal-case overflow-ellipsis overflow-hidden text-black transition-all'}).get_text().strip()\n",
        "        except:\n",
        "            type_spec = 'N/A'\n",
        "\n",
        "        try:\n",
        "            # Get the product warranty information\n",
        "            warranty = item.find('span', {'class': 'body-2-light text-black'}).get_text().strip()\n",
        "        except:\n",
        "            warranty = 'N/A'\n",
        "\n",
        "        try:\n",
        "            # Get the discounted price\n",
        "            discount_price = item.find('span', {'class': 'body-2-bold text-black'}).get_text().strip()\n",
        "        except:\n",
        "            discount_price = 'N/A'\n",
        "\n",
        "        try:\n",
        "            # Get the actual price (original price)\n",
        "            original_price = item.find('div', {'class': 'body-2-light text-primary-light line-through'}).get_text().strip()\n",
        "        except:\n",
        "            original_price = 'N/A'\n",
        "\n",
        "        # Append the extracted data to respective lists\n",
        "        names.append(name)\n",
        "        types.append(type_spec)\n",
        "        warranties.append(warranty)\n",
        "        discount_prices.append(discount_price)\n",
        "        original_prices.append(original_price)\n",
        "\n",
        "# Create a DataFrame to store the extracted data\n",
        "data_frame = pd.DataFrame({'Name': names, 'Type/Specification': types, 'Warranty Info': warranties,\n",
        "                           'Discount Price': discount_prices, 'Original Price': original_prices})\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "excel_filename = \"extracted_data.xlsx\"\n",
        "data_frame.to_excel(excel_filename, index=False)\n",
        "\n",
        "# Display the DataFrame and the Excel filename\n",
        "print(f\"Data has been saved to {excel_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px6wgvog9-pa"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2013-2023).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5rjlclf9-pb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05b64a38-e493-43f4-acca-c01c3340763d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 500 articles and saved to 'articles.json'.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "def fetch_google_scholar_articles(query, start_year, end_year, num_articles):\n",
        "    base_url = \"https://scholar.google.com/scholar\"\n",
        "    articles = []\n",
        "    articles_per_page = 20\n",
        "\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.1234.0 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "    for start in range(0, num_articles, articles_per_page):\n",
        "        params = {\n",
        "            \"q\": query,\n",
        "            \"as_ylo\": start_year,\n",
        "            \"as_yhi\": end_year,\n",
        "            \"start\": start\n",
        "        }\n",
        "\n",
        "        response = requests.get(base_url, params=params, headers=headers)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(\"Failed to retrieve some articles.\")\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        results = soup.find_all('div', {'class': 'gs_ri'})\n",
        "\n",
        "        for result in results:\n",
        "            title = result.find('h3', {'class': 'gs_rt'}).text if result.find('h3', {'class': 'gs_rt'}) else None\n",
        "            venue_info = result.find('div', {'class': 'gs_a'}).text.split('-') if result.find('div', {'class': 'gs_a'}) else [None, None]\n",
        "            authors, year = venue_info[0].strip(), venue_info[-1].strip()\n",
        "            abstract = result.find('div', {'class': 'gs_rs'}).text if result.find('div', {'class': 'gs_rs'}) else None\n",
        "\n",
        "            articles.append({\n",
        "                \"title\": title,\n",
        "                \"venue\": year,\n",
        "                \"year\": year,\n",
        "                \"authors\": authors,\n",
        "                \"abstract\": abstract\n",
        "            })\n",
        "\n",
        "            if len(articles) >= num_articles:\n",
        "                return articles\n",
        "\n",
        "    return articles\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    articles = fetch_google_scholar_articles(\"information retrieval\", 2000, 2023, 1000)\n",
        "\n",
        "    with open(\"articles.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
        "        json.dump(articles, json_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"Collected {len(articles)} articles and saved to 'articles.json'.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do either of the question-4 tasks given below."
      ],
      "metadata": {
        "id": "yCQpbJnwTxAB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT3CNj_V9-pb"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data.\n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FymVNKVi9-pb"
      },
      "outputs": [],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4 (10 points):\n",
        "\n",
        "In this task, you are required to identify and utilize online tools for web scraping data from websites without the need for coding, with a specific focus on Parsehub. The objective is to gather data and save it in formats like CSV, Excel, or any other suitable file format.\n",
        "\n",
        "You have to mention an introduction to the tool which ever you prefer to use, steps to follow for web scrapping and the final output of the data collected.\n",
        "\n",
        "Upload a document (Word or PDF File) in the same repository and you can add the link in the ipynb file."
      ],
      "metadata": {
        "id": "wOeAr9TJTBgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the link to the document here\n",
        "A=(\"https://github.com/msrahulvarma/RahulVarma_INFO5731_Fall2023/blob/main/Rahul%20varma_amezon%20scraping.pdf\")\n",
        "print(A)"
      ],
      "metadata": {
        "id": "N20TjXLmTG1u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c3f5659-28a2-4123-d82e-173a8580865f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://github.com/msrahulvarma/RahulVarma_INFO5731_Fall2023/blob/main/Rahul%20varma_amezon%20scraping.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Ts-t3cu-Hip"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}